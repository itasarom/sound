{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import tqdm\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conf:\n",
    "    # Preprocessing settings\n",
    "    sampling_rate = 44100\n",
    "    duration = 2\n",
    "    hop_length = 347*duration # to make time steps 128\n",
    "    fmin = 20\n",
    "    fmax = sampling_rate // 2\n",
    "    n_mels = 128\n",
    "    n_fft = n_mels * 20\n",
    "    samples = sampling_rate * duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция загрузки и показа аудио\n",
    "\n",
    "def load(config, path):\n",
    "    signal, _ = librosa.load(path, sr=config.sampling_rate)\n",
    "    return signal\n",
    "\n",
    "def show(audio):\n",
    "    return Audio(audio, rate=sample_rate)\n",
    "\n",
    "# будем эксперементировать с этим файлом\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_overall_lwlrap_sklearn(truth, scores):\n",
    "    \"\"\"Calculate the overall lwlrap using sklearn.metrics.lrap.\"\"\"\n",
    "    # sklearn doesn't correctly apply weighting to samples with no labels, so just skip them.\n",
    "    sample_weight = np.sum(truth > 0, axis=1)\n",
    "    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
    "    overall_lwlrap = label_ranking_average_precision_score(\n",
    "        truth[nonzero_weight_sample_indices, :] > 0, \n",
    "        scores[nonzero_weight_sample_indices, :], \n",
    "        sample_weight=sample_weight[nonzero_weight_sample_indices])\n",
    "    return overall_lwlrap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "def read_audio(conf, pathname, trim_long_data):\n",
    "    y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n",
    "    # trim silence\n",
    "    if 0 < len(y): # workaround: 0 length causes error\n",
    "        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n",
    "    # make it unified length to conf.samples\n",
    "    if len(y) > conf.samples: # long enough\n",
    "        if trim_long_data:\n",
    "#             y = y[0:0+123121523]\n",
    "#             print(conf.samples, len(y))\n",
    "            y = y[0:0+conf.samples]\n",
    "    else: # pad blank\n",
    "        padding = conf.samples - len(y)    # add padding at both ends\n",
    "        offset = padding // 2\n",
    "        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n",
    "    return y\n",
    "\n",
    "\n",
    "def audio_to_melspectrogram(conf, audio):\n",
    "    spectrogram = librosa.feature.melspectrogram(audio, \n",
    "                                                 sr=conf.sampling_rate,\n",
    "                                                 n_mels=conf.n_mels,\n",
    "                                                 hop_length=conf.hop_length,\n",
    "                                                 n_fft=conf.n_fft,\n",
    "                                                 fmin=conf.fmin,\n",
    "                                                 fmax=conf.fmax)\n",
    "    spectrogram = librosa.power_to_db(spectrogram)\n",
    "    spectrogram = spectrogram.astype(np.float32)\n",
    "    return spectrogram\n",
    "\n",
    "def show_melspectrogram(conf, mels, title='Log-frequency power spectrogram'):\n",
    "    librosa.display.specshow(mels, x_axis='time', y_axis='mel', \n",
    "                             sr=conf.sampling_rate, hop_length=conf.hop_length,\n",
    "                            fmin=conf.fmin, fmax=conf.fmax)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n",
    "    x = read_audio(conf, pathname, trim_long_data)\n",
    "    mels = audio_to_melspectrogram(conf, x)\n",
    "    if debug_display:\n",
    "        IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n",
    "        show_melspectrogram(conf, mels)\n",
    "    return mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = load(\"./data/train_curated/21082fb0.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mels = audio_to_melspectrogram(conf, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_melspectrogram(conf, mels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.read_csv(\"./data/sample_submission.csv\").columns[1:]\n",
    "column_encoder = {c:id for id, c in enumerate(columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path, config, transform):\n",
    "    meta = pd.read_csv(path + \".csv\")\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    max_len = 0\n",
    "    \n",
    "    for id, (f, label) in tqdm.tqdm(enumerate(zip(meta.fname, meta.labels)), total=len(meta)):\n",
    "        all_labels.append(label)\n",
    "        all_data.append(transform(read_audio(config, os.path.join(path, f), trim_long_data=True)))\n",
    "        max_len = max(all_data[-1].shape[-1], max_len)\n",
    "#         if len(all_data) > 1:\n",
    "#             break\n",
    "    \n",
    "    y = np.zeros((len(all_data), len(columns)))\n",
    "    for id, label in enumerate(all_labels):\n",
    "        labels = label.split(\",\")\n",
    "        for l in labels:\n",
    "            y[id, column_encoder[l]] = 1.0\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    return all_data, y, max_len\n",
    "\n",
    "def read_test(path, config, transform):\n",
    "    all_data = []\n",
    "    max_len = 0\n",
    "    meta = list(os.listdir(path))\n",
    "    meta.sort()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for id, f in tqdm.tqdm(enumerate(meta), total=len(meta)):\n",
    "        all_data.append(transform(read_audio(config, os.path.join(path, f), trim_long_data=True)))\n",
    "        max_len = max(all_data[-1].shape[-1], max_len)\n",
    "        \n",
    "    y = np.zeros((len(all_data), len(columns)))\n",
    "    \n",
    "    return meta, all_data, y, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4970/4970 [06:15<00:00, 13.24it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data, all_y, max_len = read_dataset(\"./data/train_curated\", conf, lambda s: audio_to_melspectrogram(conf, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMap(nn.Module):\n",
    "    \"\"\" A Layer which provides attention map for a query. \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.query_dim = query_dim\n",
    "    \n",
    "    def forward(self, features, mask=None):\n",
    "        \"\"\"\n",
    "        features: B, T, D\n",
    "        mask: B, T\n",
    "        -> B, T, T\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = torch.einsum(\"btd,bed->bte\",(features, features))\n",
    "        weights = F.softmax(weights, dim=2)\n",
    "        if mask is not None:\n",
    "            weights = weights * mask.unsqueeze(dim=1)\n",
    "            weights /= weights.sum(dim=2, keepdim=True)\n",
    "        \n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerWithSelfAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden = hidden_size\n",
    "        self.attention_map = AttentionMap()\n",
    "        self.cell = nn.LSTM(input_size, hidden_size, dropout=dropout, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, input, mask):\n",
    "        cell_output, _ = self.cell(input)\n",
    "        \n",
    "        attention_weights = self.attention_map(cell_output, mask)\n",
    "        output = (cell_output.unsqueeze(1) * attention_weights.unsqueeze(-1)).sum(dim=2)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.attention_map = AttentionMap()\n",
    "    \n",
    "    \n",
    "    def forward(self, input, mask=None):\n",
    "        \n",
    "        \n",
    "        attention_weights = self.attention_map(input, mask)\n",
    "        output = (input.unsqueeze(1) + input.unsqueeze(1) * attention_weights.unsqueeze(-1)).sum(dim=2)\n",
    "        output = F.dropout(output, self.dropout)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, all_data, y):\n",
    "        super().__init__()\n",
    "        self.all_data = all_data\n",
    "        self.y = y\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.all_data[idx].T\n",
    "        result = {\n",
    "            \"x\":x,\n",
    "            \"y\":self.y[idx],\n",
    "            \"mask\":np.ones(x.shape[:-1])\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    x = []\n",
    "    y = []\n",
    "    mask = []\n",
    "    \n",
    "    for s in samples:\n",
    "        x.append(s[\"x\"][None])\n",
    "        y.append(s[\"y\"][None])\n",
    "        mask.append(s[\"mask\"][None])\n",
    "    \n",
    "    \n",
    "    x = np.concatenate(x, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "    mask = np.concatenate(mask, axis=0)\n",
    "    \n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.Tensor(y)\n",
    "    mask = torch.Tensor(mask)\n",
    "    \n",
    "    return dict(x=x, y=y, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "train_x, test_x, train_y, test_y = train_test_split(all_data, all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SoundDataset(train_x, train_y)\n",
    "test_dataset = SoundDataset(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_dims, n_classes):\n",
    "#         super().__init__()\n",
    "#         self.input_dims = input_dims\n",
    "#         self.n_classes = n_classes\n",
    "        \n",
    "        \n",
    "#         self.attention_layers = nn.ModuleList([\n",
    "#             LayerWithSelfAttention(input_dims, 128, dropout=0.0),\n",
    "#             LayerWithSelfAttention(256, 128, dropout=0.0),\n",
    "#             LayerWithSelfAttention(256, 128, dropout=0.0),\n",
    "#             ]\n",
    "#         )\n",
    "        \n",
    "#         self.num_layers = 3\n",
    "#         self.num_dir = 2\n",
    "#         self.hidden_dim = 128\n",
    "#         self.last_lstm = nn.LSTM(256, self.hidden_dim, num_layers=self.num_layers, batch_first=True, bidirectional=(self.num_dir==2))\n",
    "        \n",
    "#         self.classifier_net = nn.Sequential(\n",
    "#             nn.Linear(self.hidden_dim * self.num_layers * self.num_dir, 512),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(256, self.n_classes)\n",
    "#         )\n",
    "        \n",
    "    \n",
    "#     def forward(self, input, mask):\n",
    "#         batch_size = input.shape[0]\n",
    "#         x = input\n",
    "#         for l in self.attention_layers:\n",
    "#             x = l(x, mask)\n",
    "#             x = F.dropout(x, 0.5)\n",
    "        \n",
    "#         _, (h, c) = self.last_lstm(x)\n",
    "        \n",
    "# #         print(h.shape)\n",
    "#         h = h.view(self.num_layers, self.num_dir, batch_size, -1)\n",
    "#         h = h.permute(2, 1, 0, 3)\n",
    "#         h = h.reshape(batch_size, -1)\n",
    "        \n",
    "#         classes = self.classifier_net(h)\n",
    "        \n",
    "#         return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dims, n_classes):\n",
    "        super().__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        \n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            SimpleSelfAttention(dropout=0.0),\n",
    "            nn.LeakyReLU(),\n",
    "            SimpleSelfAttention(dropout=0.0),\n",
    "            nn.LeakyReLU(),\n",
    "            SimpleSelfAttention(dropout=0.0),\n",
    "            nn.LeakyReLU(),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers = 3\n",
    "        self.num_dir = 2\n",
    "        self.hidden_dim = 128\n",
    "        \n",
    "#         self.flatten\n",
    "#         self.last_lstm = nn.LSTM(128, self.hidden_dim, num_layers=self.num_layers, batch_first=True, bidirectional=(self.num_dir==2))\n",
    "        \n",
    "        self.classifier_net = nn.Sequential(\n",
    "            nn.Linear(128 * 128, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, self.n_classes)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, input, mask):\n",
    "        batch_size = input.shape[0]\n",
    "        x = input\n",
    "        for l in self.attention_layers:\n",
    "            x = l(x)\n",
    "\n",
    "        x = x.reshape(batch_size, -1)\n",
    "#         _, (h, c) = self.last_lstm(x)\n",
    "        \n",
    "# #         print(h.shape)\n",
    "#         h = h.view(self.num_layers, self.num_dir, batch_size, -1)\n",
    "#         h = h.permute(2, 1, 0, 3)\n",
    "#         h = h.reshape(batch_size, -1)\n",
    "        \n",
    "        classes = self.classifier_net(x)\n",
    "        \n",
    "        return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_function(logits, y):\n",
    "#     pred = torch.sigmoid(logits)\n",
    "#     res = (torch.log(pred) * y + torch.log(1 - pred) * (1 - y)).mean(dim=-1)\n",
    "#     return -res.mean()\n",
    "\n",
    "# def loss_function(logits, y):\n",
    "#     pred = torch.sigmoid(logits)\n",
    "#     res = (torch.log(pred) * y + torch.log(1 - pred) * (1 - y)).mean(dim=-1)\n",
    "#     return -res.mean()\n",
    "\n",
    "# def loss_function(logits, y):\n",
    "#     pred = torch.sigmoid(logits)\n",
    "#     res = (torch.log(pred) * y).sum(dim=-1)\n",
    "#     return -res.mean()\n",
    "\n",
    "def loss_function(logits, y):\n",
    "    pred = F.log_softmax(logits, dim=1)\n",
    "    res = (pred * y).sum(dim=-1)\n",
    "    return -res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    target_metrics = 0\n",
    "    total_x = 0\n",
    "    for b in loader:\n",
    "        x = b['x'].to(device)\n",
    "        mask = b['mask'].to(device)\n",
    "        y = b['y'].to(device)\n",
    "        logits = model.forward(x, mask)\n",
    "        loss = loss_function(logits, y)\n",
    "        losses += loss.item() * len(x)\n",
    "        target_metric = calculate_overall_lwlrap_sklearn(y.cpu().numpy(), logits.detach().cpu().numpy())\n",
    "        target_metrics += target_metric * len(x)\n",
    "        total_x += len(x)\n",
    "        \n",
    "    losses /= total_x\n",
    "    target_metrics /= total_x\n",
    "    \n",
    "    return {\"loss\":losses, \"target_metric\":target_metrics}\n",
    "\n",
    "\n",
    "def plot(train_metrics, val_metrics):\n",
    "    display.clear_output()\n",
    "    for key in train_metrics:\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.title(key)\n",
    "        plt.plot(train_metrics[key], label='train_' + key)\n",
    "        plt.plot(val_metrics['iterations'], val_metrics[key], label='val_' + key)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = Net(128, 80)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# target_metrics = []\n",
    "\n",
    "train_metrics = defaultdict(list)\n",
    "val_metrics = defaultdict(list)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-f7e462c0686c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_iterations\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-125-823d5732cca1>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(train_metrics, val_metrics)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mtitle\u001b[0;34m(s, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \"\"\"\n\u001b[0;32m-> 1427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[0;31m## Axis ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mgca\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \"\"\"\n\u001b[0;32m--> 984\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;31m# More ways of creating axes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0;31m# no axes found, so create one which spans the figure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;31m# add a layout box to this, for both the full axis, and the poss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# of the axis.  We need both because the axes may become smaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# a dict from events to (id, func)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;31m# funcs used to format x and y - fall back on major formatters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mcla\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# update the minor locator for x and y axis based on rcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xtick.minor.visible'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_minor_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmticker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoMinorLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_iterations = 0\n",
    "for i in range(100):\n",
    "    for b in train_loader:\n",
    "        model.train()\n",
    "        \n",
    "        x = b['x'].to(device)\n",
    "        mask = b['mask'].to(device)\n",
    "        y = b['y'].to(device)\n",
    "        logits = model.forward(x, mask)\n",
    "        loss = loss_function(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        nn.utils.clip_grad.clip_grad_norm_(model.parameters(), 10.0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_metrics['loss'].append(loss.item())\n",
    "        target_metric = calculate_overall_lwlrap_sklearn(y.cpu().numpy(), logits.detach().cpu().numpy())\n",
    "        train_metrics['target_metric'].append(target_metric)\n",
    "        \n",
    "\n",
    "        plot(train_metrics, val_metrics)\n",
    "        \n",
    "        if total_iterations % 10:\n",
    "            validation_result = validate(model, test_loader)\n",
    "            \n",
    "            val_metrics['iterations'].append(total_iterations)\n",
    "            for key, value in validation_result.items():\n",
    "                val_metrics[key].append(value)\n",
    "                \n",
    "        total_iterations += 1\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-87336408., device='cuda:0', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.log_softmax(logits, dim=1) * y).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_metrics, val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model_weights.tc\"))\n",
    "# model = model.to(device)\n",
    "\n",
    "# torch.save(model.cpu().state_dict(), \"model_weights_2.tc\")\n",
    "# torch.save((train_metrics, val_metrics), \"metrics_2.tc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for b in loader:\n",
    "        x = b['x'].to(device)\n",
    "        mask = b['mask'].to(device)\n",
    "#         pred = F.softmax(model.forward(x, mask), dim=1)\n",
    "        pred = model.forward(x, mask)\n",
    "        \n",
    "        result.append(pred.detach().cpu().numpy())\n",
    "        \n",
    "    \n",
    "    result = np.concatenate(result, axis=0)\n",
    "    \n",
    "    return result\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 88/1120 [00:01<00:21, 47.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-e247e909304f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maudio_to_melspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-49d0b8dc4955>\u001b[0m in \u001b[0;36mread_test\u001b[0;34m(path, config, transform)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_long_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-e247e909304f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maudio_to_melspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-4218dbe1e116>\u001b[0m in \u001b[0;36maudio_to_melspectrogram\u001b[0;34m(conf, audio)\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                  \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                  \u001b[0mfmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                  fmax=conf.fmax)\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mspectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mspectrogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectrogram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, power, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0mmel_basis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_basis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ids, all_data, all_y, max_len = read_test(\"./data/test\", conf, lambda s: audio_to_melspectrogram(conf, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataset = SoundDataset(all_data, all_y)\n",
    "result_loader = DataLoader(result_dataset, batch_size=128, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(model, result_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_prediction = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {col:prediction[:, id] for id, col in enumerate(columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['fname'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.set_index('fname').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"my_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.929913 ,  -3.6036205,   4.266451 , ..., -21.630527 ,\n",
       "        -7.1434326,   2.1747646], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -9.929913\n",
       "1       -3.603621\n",
       "2        4.266451\n",
       "3       -2.209997\n",
       "4        3.432890\n",
       "5       -3.077633\n",
       "6      -22.607225\n",
       "7      -11.041691\n",
       "8       -3.341435\n",
       "9       -7.098019\n",
       "10      -5.419654\n",
       "11      -9.463284\n",
       "12      -6.273924\n",
       "13     -12.217808\n",
       "14      -5.011210\n",
       "15      -5.959308\n",
       "16      -9.098918\n",
       "17      -8.775235\n",
       "18       0.366376\n",
       "19      -3.345796\n",
       "20       2.948148\n",
       "21       1.187523\n",
       "22      -7.192244\n",
       "23       9.265618\n",
       "24       1.164975\n",
       "25      -3.827711\n",
       "26      -4.770999\n",
       "27     -10.357783\n",
       "28     -11.072790\n",
       "29       2.469971\n",
       "          ...    \n",
       "1090    -5.064929\n",
       "1091    -2.207104\n",
       "1092     2.104628\n",
       "1093     4.463598\n",
       "1094     1.008486\n",
       "1095     3.886524\n",
       "1096    -4.227357\n",
       "1097    -5.743256\n",
       "1098     6.340885\n",
       "1099    -2.535640\n",
       "1100     3.765434\n",
       "1101    -3.036433\n",
       "1102     3.247590\n",
       "1103     1.732551\n",
       "1104   -10.396237\n",
       "1105     3.969244\n",
       "1106    -6.886586\n",
       "1107    -6.288598\n",
       "1108    -3.855924\n",
       "1109   -16.712967\n",
       "1110    -2.593570\n",
       "1111    -5.543413\n",
       "1112    -6.746543\n",
       "1113     1.845442\n",
       "1114    -2.289312\n",
       "1115    -0.517549\n",
       "1116    -3.900541\n",
       "1117   -21.630527\n",
       "1118    -7.143433\n",
       "1119     2.174765\n",
       "Name: Accelerating_and_revving_and_vroom, Length: 1120, dtype: float32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Accelerating_and_revving_and_vroom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname,Accelerating_and_revving_and_vroom,Accordion,Acoustic_guitar,Applause,Bark,Bass_drum,Bass_guitar,Bathtub_(filling_or_washing),Bicycle_bell,Burping_and_eructation,Bus,Buzz,Car_passing_by,Cheering,Chewing_and_mastication,Child_speech_and_kid_speaking,Chink_and_clink,Chirp_and_tweet,Church_bell,Clapping,Computer_keyboard,Crackle,Cricket,Crowd,Cupboard_open_or_close,Cutlery_and_silverware,Dishes_and_pots_and_pans,Drawer_open_or_close,Drip,Electric_guitar,Fart,Female_singing,Female_speech_and_woman_speaking,Fill_(with_liquid),Finger_snapping,Frying_(food),Gasp,Glockenspiel,Gong,Gurgling,Harmonica,Hi-hat,Hiss,Keys_jangling,Knock,Male_singing,Male_speech_and_man_speaking,Marimba_and_xylophone,Mechanical_fan,Meow,Microwave_oven,Motorcycle,Printer,Purr,Race_car_and_auto_racing,Raindrop,Run,Scissors,Screaming,Shatter,Sigh,Sink_(filling_or_washing),Skateboard,Slam,Sneeze,Squeak,Stream,Strum,Tap,Tick-tock,Toilet_flush,Traffic_noise_and_roadway_noise,Trickle_and_dribble,Walk_and_footsteps,Water_tap_and_faucet,Waves_and_surf,Whispering,Writing,Yell,Zipper_(clothing)\r\n",
      "000ccb97.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n",
      "0012633b.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n",
      "001ed5f1.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n",
      "00294be0.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n",
      "003fde7a.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n",
      "0040ccc9.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n",
      "0046b732.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n",
      "004f3bbc.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n",
      "00526050.wav,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\r\n"
     ]
    }
   ],
   "source": [
    "!head data/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fname,Accelerating_and_revving_and_vroom,Accordion,Acoustic_guitar,Applause,Bark,Bass_drum,Bass_guitar,Bathtub_(filling_or_washing),Bicycle_bell,Burping_and_eructation,Bus,Buzz,Car_passing_by,Cheering,Chewing_and_mastication,Child_speech_and_kid_speaking,Chink_and_clink,Chirp_and_tweet,Church_bell,Clapping,Computer_keyboard,Crackle,Cricket,Crowd,Cupboard_open_or_close,Cutlery_and_silverware,Dishes_and_pots_and_pans,Drawer_open_or_close,Drip,Electric_guitar,Fart,Female_singing,Female_speech_and_woman_speaking,Fill_(with_liquid),Finger_snapping,Frying_(food),Gasp,Glockenspiel,Gong,Gurgling,Harmonica,Hi-hat,Hiss,Keys_jangling,Knock,Male_singing,Male_speech_and_man_speaking,Marimba_and_xylophone,Mechanical_fan,Meow,Microwave_oven,Motorcycle,Printer,Purr,Race_car_and_auto_racing,Raindrop,Run,Scissors,Screaming,Shatter,Sigh,Sink_(filling_or_washing),Skateboard,Slam,Sneeze,Squeak,Stream,Strum,Tap,Tick-tock,Toilet_flush,Traffic_noise_and_roadway_noise,Trickle_and_dribble,Walk_and_footsteps,Water_tap_and_faucet,Waves_and_surf,Whispering,Writing,Yell,Zipper_(clothing)\r\n",
      "000ccb97.wav,-9.929913,-14.2122965,-13.610633,-13.858889,-1.7857001,-4.992347,-12.33691,-3.7084503,-6.1317277,-7.444166,-11.204617,-2.65557,-11.774877,-14.398441,-4.9689393,-13.662802,4.333856,-10.774259,-2.3938444,-0.9049307,1.3862673,-1.1712171,-7.4410152,-18.111122,-0.04113138,2.0744061,2.8179486,-3.6217105,-4.956403,-2.7353194,-5.4819365,-16.999542,-6.568117,4.1827497,1.3764733,-9.264893,2.1842525,-1.5757041,-3.8359375,-3.5825162,-13.0261545,-5.400469,-0.31193244,-3.8442824,-6.2003226,-16.516857,-4.453878,1.1208481,-9.486447,-2.8162084,-5.161074,-6.7333384,-3.9355977,-7.766297,-15.02324,-2.1746478,-8.236039,0.9293173,-5.761521,7.022254,-3.1922748,-1.0201019,-3.5622902,-2.6547244,2.418964,-2.633661,-15.214613,-14.603335,3.7704246,2.6169188,-15.741083,-12.506939,-5.5218062,0.72380227,-1.4058363,-18.665659,-9.046217,-3.854371,-4.1943517,-1.5406178\r\n",
      "0012633b.wav,-3.6036205,-7.335658,0.8841078,-6.3938103,-3.078323,-5.9304166,-8.386244,-2.0724063,-12.736001,-2.0680532,-5.563036,-1.3203906,-13.261904,-5.0639815,-4.276523,-5.121492,-3.7865305,-5.4543343,-8.463674,-5.832379,-0.7397003,-7.9197345,-3.434565,-8.327254,-4.157202,-8.627643,-2.1887267,-2.0526743,2.2388487,-3.2996547,-3.5141256,-5.630245,-5.102471,-3.5978801,-7.0895467,1.0153701,-4.9593563,-12.971427,-8.942612,2.9717658,-8.041204,-9.96166,-9.229467,-8.247735,0.32614097,-4.793505,0.2923048,-3.0460126,-10.179121,-4.776505,-2.3336318,-5.736006,-0.84352076,-1.5255854,-9.189895,-5.222508,0.25945243,-5.5991116,-8.491224,-6.03514,-4.098983,2.5914867,-4.5134315,-4.220641,-5.9676538,-0.92934614,-2.5422678,-5.659346,-2.1896713,0.22219267,2.162972,-10.139823,-1.3720496,3.06876,1.7194974,-7.079544,-3.258934,0.55251133,-6.6596427,-1.1795403\r\n",
      "001ed5f1.wav,4.266451,-3.0021276,-8.495674,-5.453654,-0.6265874,-5.8595114,-5.876081,-3.8528624,-6.817615,-3.8118215,0.3407957,0.09873453,5.81508,-4.931621,-7.093361,-2.912321,-11.061698,1.2937838,2.2118583,-3.3659236,-3.2880697,-10.229789,1.4745817,-4.1067333,-3.5586183,-7.26297,-5.8951344,-1.421298,-9.515216,-7.281591,-8.059141,-2.816126,-1.6291302,-4.0973573,-12.608665,-8.029761,-10.092184,-12.081381,0.48492914,-7.6332865,-6.955663,-12.31232,-2.0294147,-6.854113,-6.537628,-5.272702,-4.759563,-6.7719994,3.5087945,-0.80354965,2.216931,4.256285,0.485544,3.0382352,3.1106179,-12.160781,-2.511176,-9.833213,-5.7981224,-13.408811,-5.459398,-0.5035703,-0.30132124,-2.3091357,-0.0014690682,-0.9359192,0.90806925,-5.298932,-6.006785,-5.156774,-0.33292058,4.714952,-5.217587,0.17933118,-0.6783661,2.1864586,-2.0651932,-7.1543436,-4.96249,-0.37596497\r\n",
      "00294be0.wav,-2.2099967,-6.43329,-6.9361005,-6.5730515,-1.6886264,-8.635262,-8.263404,0.33189863,-0.025654543,-1.2977817,-0.68585205,1.2833897,-1.6558336,-7.736924,0.27232036,-0.40585735,-1.614385,-0.31964257,-2.3548033,-3.8869112,1.8375541,0.64120036,-0.84834135,-8.948107,-2.1896389,-0.017893966,-0.23729022,1.3822705,-2.57905,-6.1263747,-4.9361916,-0.3639952,-3.24428,-0.79092634,-4.7350364,-3.227771,-4.21658,-6.5128517,-2.8094535,-2.030456,-1.2930264,-6.6388683,-0.5778162,-1.8673906,-1.8521056,-6.306435,-1.31408,-4.4008803,-3.096505,1.7918339,0.080327824,-0.9758719,-0.6805811,1.3888515,-3.2640758,-3.7898872,0.18455817,0.31477454,-6.073339,-7.5923862,0.8312251,-0.6047427,-2.46148,0.28227243,-1.1486661,-1.4024578,-0.31289375,-9.001272,-0.41604814,0.04460928,-0.25912017,-1.238774,-3.1146424,0.18955387,-0.5689988,-0.08398691,-1.6115026,1.4097474,-5.54451,1.8511022\r\n",
      "003fde7a.wav,3.4328897,0.39622614,-3.2623467,-1.8785148,-1.9903606,-2.1583078,-4.14924,-5.807317,-6.919071,-1.3743018,1.4532082,0.011782173,2.075548,-1.6890382,-8.4580145,-2.618643,-6.7861342,-1.1896969,1.2770109,-0.108379535,0.26209712,-8.345128,0.8601641,1.1549306,-1.6171347,-6.253777,-4.1108603,-3.3878736,-6.0599256,-2.4998553,-4.2515054,-1.3136281,-1.8538545,-1.3297874,-9.177095,-5.054637,-4.020098,-9.351241,0.077612646,-5.094781,-3.181925,-8.365112,-0.7024337,-8.339579,-4.1660347,-1.395649,-1.1166362,-4.7517414,0.71834356,-1.6665655,1.2999651,2.8542523,-0.44736752,1.3753848,4.115259,-10.690704,-1.355448,-7.59147,-1.5232235,-7.7149553,-6.115033,-2.2577713,0.43576047,-1.2996348,-0.58012784,-0.44181445,1.1379592,-1.6315086,-6.530024,-2.1181948,-0.06735718,2.1771197,-3.786348,-0.103347704,-1.2392946,0.75630146,1.9375392,-5.469134,-2.9686263,0.43297252\r\n",
      "0040ccc9.wav,-3.0776331,-14.168364,-2.7730882,-10.474797,-1.3235759,-3.075894,-5.566958,-1.037921,-5.0258007,-4.169643,-3.5253243,-2.2964263,-6.2795644,-12.175845,-1.2380089,-5.921195,-0.9732593,-2.4209075,-3.7022877,-5.742895,-0.25931737,0.57667774,1.926343,-12.539333,2.6253924,-6.3776383,-2.4769974,1.3527739,-0.37112385,-7.447684,-7.176193,-4.335375,-5.975448,-3.1726627,-8.7993765,-2.9991539,-8.711466,-10.426922,-2.9732914,1.2833434,-9.588693,-8.623565,-3.400494,-3.6939523,1.8482424,-10.962949,-1.3743347,-5.7633247,-0.11853933,-2.0519166,-0.015869498,-2.8833623,-0.3295521,2.4541929,-4.6134224,-0.49590182,2.4389913,0.9104011,-14.024208,-8.701663,-1.9108964,-0.012295339,-4.527503,0.87942165,-3.8513126,1.9797608,-6.8972974,-6.9726954,1.869657,-1.2015084,-1.394222,-5.3562245,-7.5683546,2.7917128,-1.2787555,-3.2296166,-3.1435654,-0.079081826,-12.874138,-1.1335498\r\n",
      "0046b732.wav,-22.607225,-18.77312,-16.96014,-19.338638,-4.5853844,1.9312714,-16.43709,-4.3912826,7.4747524,-8.429101,-4.7503934,-0.4033143,-13.654628,-17.51303,-7.2326117,-10.713682,5.560624,0.8811359,-3.239932,2.742214,0.48721203,-0.92239463,1.3366859,-25.58804,-5.54509,2.3239396,4.801117,-0.8240136,4.003043,-6.8710647,-12.033042,-17.367304,-10.906454,1.6030079,11.712412,-9.156775,3.5975873,10.523451,-2.6620176,-5.248108,-16.055088,-0.46434525,-0.8092546,-6.431621,-1.1542375,-17.368877,-3.24254,12.4779,-15.439519,-4.330379,1.6485372,-5.2207747,-4.757056,-4.9447503,-20.448532,4.953034,-10.783887,2.4560425,-7.430589,6.201947,-3.6768682,-5.3715386,-11.058287,-2.0485198,-1.5639628,-6.1620975,-14.9424925,-21.20869,7.7032547,3.5282676,-12.215972,-8.898409,-5.197455,-8.324312,-0.20580783,-19.423729,-4.387372,0.39763963,-7.538068,1.0433172\r\n",
      "004f3bbc.wav,-11.041691,-7.722928,-21.936241,-9.955171,-4.578496,-23.455803,-19.980003,-5.69679,-8.673395,-14.039634,-14.777436,-6.3035703,-20.869007,-11.45514,-7.1134076,-4.39219,-1.8905576,-8.440238,-3.8545296,-4.7214146,-11.7992,2.6222405,-11.919425,-7.108621,-11.873694,-5.974457,-4.7438235,-16.658157,-2.616114,-4.1940546,-7.5571103,2.0135624,-1.831783,-1.7724364,-16.110336,-11.4340315,0.959168,-14.781408,-1.7936246,-8.240971,-1.836118,-3.093062,-0.33133477,-8.570632,-18.066132,-7.5828795,-6.622258,1.013549,-20.201288,5.6965175,-13.986387,-12.426581,-7.43765,-3.0025744,-15.426572,-5.486192,-13.144896,-8.918289,6.242746,-9.729598,1.0962195,-6.6381288,-7.1332474,0.5048758,-1.4123167,-0.23696414,-10.637277,-19.217152,-11.544364,-11.379519,-10.645158,-10.626596,2.6801243,-14.664255,-8.209425,-11.63196,-4.3136683,-3.0478756,3.5544922,1.1127722\r\n",
      "00526050.wav,-3.3414352,6.7854934,-14.806599,-6.5487285,-12.898506,-18.029215,-14.726292,2.0007703,-10.191205,0.75077665,-8.878385,-2.6302812,-16.523338,-6.541948,-4.534238,-2.8411934,-11.853398,-2.6199107,-13.361508,-8.1121235,-7.1827826,-1.788119,-6.003437,-6.2028456,-3.5194137,-4.19462,-7.8444085,-11.864001,-6.6519566,0.08677094,-4.109226,1.3708456,-1.4341673,-1.2286502,-10.972154,-0.21282454,-8.08245,-15.594901,-12.883354,-1.5220101,3.6824126,-8.7937,4.0583916,-5.179989,-13.783463,-2.1378725,-6.6992464,-6.602819,-12.238569,-3.035535,-12.767447,-9.42979,-2.6809058,-5.2552466,-16.811172,-11.436901,-3.7035823,-3.2686975,-8.205888,-17.891088,-0.36711887,-0.19907989,-11.579593,-1.0390865,-11.117736,1.7000245,-0.90680397,-14.140983,-12.656069,-3.4577975,2.0446181,-6.9013615,-1.3195012,-1.3372266,0.51739395,-2.1362028,1.5133736,0.04287509,-2.9149423,-2.9941683\r\n"
     ]
    }
   ],
   "source": [
    "!head my_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = pd.read_csv(\"./data/train_curated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fname', 'labels'], dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
